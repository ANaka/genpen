{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17538d07-98a8-44f9-9486-b217fd684b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from dataclasses import asdict, dataclass, field\n",
    "import vsketch\n",
    "import shapely.geometry as sg\n",
    "from shapely.geometry import box, MultiLineString, Point, MultiPoint, Polygon, MultiPolygon, LineString\n",
    "import shapely.affinity as sa\n",
    "import shapely.ops as so\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import vpype_cli\n",
    "from typing import List, Generic\n",
    "from genpen import genpen as gp, utils as utils\n",
    "from scipy import stats as ss\n",
    "import geopandas\n",
    "from shapely.errors import TopologicalError\n",
    "import functools\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import vpype\n",
    "from skimage import io\n",
    "from pathlib import Path\n",
    "\n",
    "import bezier\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from skimage import feature\n",
    "from genpen.utils import Paper\n",
    "\n",
    "from scipy import spatial, stats\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shapely.geometry as sg\n",
    "from rasterio import features\n",
    "import shapely.geometry as sg\n",
    "from shapely.geometry import box, MultiLineString, Point, MultiPoint, Polygon, MultiPolygon, LineString\n",
    "import shapely.affinity as sa\n",
    "import shapely.ops as so\n",
    "from scipy import stats as ss\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from scipy import spatial, stats\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "import pytorch_lightning as pl\n",
    "from torch import tensor as Tensor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cff5ad-adfb-477c-9a7c-29430a080893",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# make page\n",
    "paper_size = '11x14 inches'\n",
    "border:float=30\n",
    "paper = Paper(paper_size)\n",
    "\n",
    "drawbox = paper.get_drawbox(border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c21155-40ac-45ea-baaf-3a1753f11ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordset_to_lines(coordset):\n",
    "    return [coords_to_line(coords) for coords in coordset]\n",
    "\n",
    "def coords_to_line(coords):\n",
    "    x0, y0, x1, y1 = coords\n",
    "    return LineString([(x0, y0), (x1, y1)])\n",
    "\n",
    "def coordset_to_img(coordset, out_shape):\n",
    "    lines = coordset_to_lines(coordset)\n",
    "    return features.rasterize(lines, out_shape=out_shape)\n",
    "\n",
    "def generate_random_line_data(n_samples, n_lines, out_shape):\n",
    "    \n",
    "    coordsets = []\n",
    "    imgs = []\n",
    "    for jj in tqdm(range(n_samples)):\n",
    "        coords = []\n",
    "        for ii in range(n_lines):\n",
    "            x0 = xgen()\n",
    "            y0 = ygen()\n",
    "            x1 = xgen()\n",
    "            y1 = ygen()\n",
    "            coords.append(np.array([x0, y0, x1, y1]))\n",
    "        coordset = np.stack(coords)\n",
    "        img = coordset_to_img(coordset, out_shape)\n",
    "        imgs.append(img)\n",
    "        coordsets.append(coords)\n",
    "    return np.stack(imgs, axis=0), np.stack(coordsets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ddad3-6a18-4f59-9171-be0c0ef8b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode(y, t, a, b, c, d):\n",
    "    v, u = y\n",
    "    dvdt = np.sin(b * u) + v * c\n",
    "    dudt = np.cos(a * v * u) + u  * d\n",
    "    dydt = [dvdt, dudt]\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb5f4b-e68f-4c2e-8fe8-48be2695b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.1\n",
    "b = 0.95\n",
    "c = - 0.02\n",
    "d = -0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1bc28c-516a-4e23-a683-2dee22fcf922",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OdeParams:\n",
    "    a = ss.norm(loc=0.05, scale=0.1)\n",
    "    b = ss.norm(loc=0.9, scale=0.1)\n",
    "    c = ss.norm(loc=-0.025, scale=0.01)\n",
    "    d = ss.norm(loc=-0.025, scale=0.01)\n",
    "    \n",
    "    def get_args(self):\n",
    "        return self.a.rvs(), self.b.rvs(), self.c.rvs(), self.d.rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7560e0-22ff-49bb-a013-38cd8591450c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589bef35-1e55-4029-af53-8ec7b29856bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_overlap_odeint(ode, pts, args, t, break_dist=0.2, min_len=0.9, verbose=False):\n",
    "    allowed_counter = 0\n",
    "    all_polys = Polygon()\n",
    "\n",
    "    break_dist = break_dist\n",
    "\n",
    "    lines = []\n",
    "    lfs = MultiLineString()\n",
    "    iterator = pts\n",
    "    if verbose:\n",
    "        iterator = tqdm(iterator)\n",
    "    for ii, pt in enumerate(iterator):\n",
    "        sol = odeint(ode, [pt.x, pt.y], t, args=args)\n",
    "        mpt = MultiPoint(sol)\n",
    "        if ii == 0:\n",
    "            ls = LineString(mpt)\n",
    "            lfs = gp.merge_LineStrings([lfs, ls])\n",
    "            lines.append(ls)\n",
    "        else:\n",
    "            allowed_counter = 0\n",
    "            for _pt in mpt:\n",
    "                dist = _pt.distance(lfs)\n",
    "                if dist < break_dist:\n",
    "                    break\n",
    "                allowed_counter += 1\n",
    "        if allowed_counter > 1:\n",
    "            ls = LineString(mpt[:allowed_counter])\n",
    "            lfs = gp.merge_LineStrings([lfs, ls])\n",
    "            lines.append(ls)\n",
    "\n",
    "    return gp.merge_LineStrings([l for l in lines if l.length > min_len])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c784b08a-ee0f-422f-8ad1-077aeba69d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def odeint_to_ls(ode, pts, args, t, min_len=0.9, verbose=False):\n",
    "    allowed_counter = 0\n",
    "    all_polys = Polygon()\n",
    "\n",
    "\n",
    "    lines = []\n",
    "    lfs = MultiLineString()\n",
    "    iterator = pts\n",
    "    if verbose:\n",
    "        iterator = tqdm(iterator)\n",
    "    for ii, pt in enumerate(iterator):\n",
    "        sol = odeint(ode, [pt.x, pt.y], t, args=args)\n",
    "        ls = LineString(sol)\n",
    "        lines.append(ls)\n",
    "\n",
    "    return gp.merge_LineStrings([l for l in lines if l.length > min_len])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6f75b8-eb67-4c1d-bfb0-0dcfe7800b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_shape=(256, 256)\n",
    "border = 5\n",
    "img_drawbox = box(border, border, out_shape[0]-border, out_shape[1]-border)\n",
    "\n",
    "n_lines = 300\n",
    "thetas = np.linspace(0, np.pi*24, n_lines)\n",
    "radii = np.linspace(0.8, 18, n_lines)\n",
    "\n",
    "pts = []\n",
    "for theta, radius in zip(thetas, radii):\n",
    "    x = np.cos(theta) * radius - 0\n",
    "    y = np.sin(theta) * radius + 0.\n",
    "    pts.append(Point(x, y))\n",
    "\n",
    "t_max = 17.7\n",
    "t = np.linspace(0, t_max, 61)\n",
    "\n",
    "linesets = []\n",
    "imgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8aacd-18e9-40ac-9035-b6e1f30c2735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for ii in tqdm(range(20000)):\n",
    "#     args = OdeParams().get_args()\n",
    "#     lss = no_overlap_odeint(ode, pts, args, t, break_dist=0.5, min_len=0.9)\n",
    "#     lss = gp.make_like(gp.merge_LineStrings(lss), img_drawbox)\n",
    "#     img = features.rasterize(lss, out_shape=out_shape)\n",
    "#     linesets.append(lss)\n",
    "#     imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b82cb44-54de-4888-83a0-b34c293a5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez_compressed('/home/naka/data/ode_vae/test.npz', imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7fbdf9-02e1-440c-9388-1cf02b07bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = OdeParams().get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8da2cf-3ea8-487b-904d-eedca0b13e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "lss = no_overlap_odeint(ode, pts, args, t, min_len=0.9, verbose=False)\n",
    "lss = gp.make_like(gp.merge_LineStrings(lss), img_drawbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9240d6b-d167-4603-b173-674f8e95ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "polys = gp.merge_Polygons([ls.buffer(1, cap_style=2, join_style=2) for ls in lss])\n",
    "img = features.rasterize(polys.boundary, out_shape=out_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6075d-ed8f-4a7d-a72f-383c7a330658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8821103-a328-4e9f-84aa-8e7090b27654",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_shape=(128, 128)\n",
    "border = 5\n",
    "img_drawbox = box(border, border, out_shape[0]-border, out_shape[1]-border)\n",
    "\n",
    "n_lines = 200\n",
    "thetas = np.linspace(0, np.pi*23.1, n_lines)\n",
    "radii = np.linspace(0.8, 18, n_lines)\n",
    "\n",
    "pts = []\n",
    "for theta, radius in zip(thetas, radii):\n",
    "    x = np.cos(theta) * radius - 0\n",
    "    y = np.sin(theta) * radius + 0.\n",
    "    pts.append(Point(x, y))\n",
    "pts = MultiPoint(pts)\n",
    "t_max = 17.7\n",
    "t = np.linspace(0, t_max, 61)\n",
    "\n",
    "a = 0.1\n",
    "b = 0.95\n",
    "c = - 0.02\n",
    "d = -0.02\n",
    "n_cs = 40\n",
    "n_ds = 40\n",
    "cs = np.geomspace(-10., -0.04, n_cs)\n",
    "ds = np.geomspace(-1., -0.04, n_ds)\n",
    "cds = list(itertools.product(cs, ds))\n",
    "n_images = len(cds)\n",
    "\n",
    "xj = ss.norm(loc=-0.3, scale=0.6).rvs\n",
    "yj = ss.norm(loc=-0.3, scale=0.6).rvs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5198f66-c074-4ec4-ad6a-ce105d2d96d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "imgs = []\n",
    "for ii in tqdm(range(n_images)):\n",
    "    c, d = cds[ii]\n",
    "    args = (a, b, c, d)\n",
    "    _pts = sa.rotate(pts, angle=np.random.uniform(0, 360))\n",
    "    _pts = sa.translate(_pts, xoff=xj(), yoff=yj())\n",
    "    lss = no_overlap_odeint(ode, _pts, args, t, break_dist=0.5, min_len=0.9)\n",
    "    lss = gp.make_like(gp.merge_LineStrings(lss), img_drawbox)\n",
    "    img = features.rasterize(lss, out_shape=out_shape)\n",
    "    linesets.append(lss)\n",
    "    imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a89e2b4-5c05-4d09-8781-a6aee103a30e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# f, axs = plt.subplots(n_cs,n_ds, figsize=(18,18))\n",
    "# axs = axs.ravel()\n",
    "# for ii in range(n_images):\n",
    "#     ax = axs[ii]\n",
    "#     ax.imshow(imgs[ii])\n",
    "#     ax.axis('off')\n",
    "    \n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43851da-c3a9-4f65-a260-2143759f7900",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('/home/naka/data/ode_vae/parametric3.npz', imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6ba033-1959-4e71-be59-615a30f16db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.load('/home/naka/data/ode_vae/parametric3.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e185ed56-8489-430e-8fa2-3790b8efdc76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, imgs, transform=None):\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.imgs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgs = self.imgs[idx, :, :]\n",
    "        imgs = transforms.ToTensor()(imgs)\n",
    "        if self.transform:\n",
    "            imgs = self.transform(imgs[:, :])\n",
    "            \n",
    "        return imgs\n",
    "    \n",
    "    \n",
    "class ImageDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        imgs,\n",
    "        train_val_test_ratio:tuple=(0.8, 0.1, 0.1),\n",
    "        batch_size=1,\n",
    "        random_state=None,\n",
    "        num_workers=0,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.imgs = imgs\n",
    "        self.random_state = random_state\n",
    "        self.train_val_test_ratio = train_val_test_ratio\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "    \n",
    "        \n",
    "    def prepare_data(self):\n",
    "        #self.data = \n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "#         split_data = train_val_test_split(self.imgs, *self.train_val_test_ratio, random_state=self.random_state)\n",
    "        self.train_data = ImageDataset(self.imgs)\n",
    "        self.val_data = ImageDataset(self.imgs)\n",
    "        self.test_data = ImageDataset(self.imgs)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data,batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e06965-5dc9-4455-a62c-fdba296e972a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class VAE(pl.LightningModule):\n",
    "    \n",
    "    def encode_unit(self, inputs, out_size, kernel_size, stride, padding=0, dropout=0):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(inputs, out_size, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_size, track_running_stats=False, affine=False),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LeakyReLU()\n",
    "            \n",
    "        )\n",
    "    \n",
    "    def decode_unit(self, inputs, out_size, kernel_size, stride, padding=0, dropout=0, output_padding=0):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(inputs, out_size, kernel_size, stride, padding, output_padding=output_padding),\n",
    "            nn.BatchNorm2d(out_size, track_running_stats=False, affine=False),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        \n",
    "        latent_dim,\n",
    "        input_size=256,\n",
    "        layer_count=3, \n",
    "        channels=1, \n",
    "        depth=2,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-5,\n",
    "        kld_loss_weight=1,\n",
    "        mul=1,\n",
    "        fc_scale=8,\n",
    "        encode_dropout=0.2,\n",
    "        decode_dropout=0.2,\n",
    "        kernel_size=4,\n",
    "        stride=2,\n",
    "        fc_size=0,\n",
    "        padding=1,\n",
    "        \n",
    "    ):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.depth = depth\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr = lr\n",
    "        self.layer_count = layer_count\n",
    "        self.weight_decay = weight_decay\n",
    "        self.kld_loss_weight = kld_loss_weight\n",
    "        self.fc_scale = fc_scale\n",
    "        self.encode_dropout = encode_dropout\n",
    "        self.decode_dropout = decode_dropout\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
    "        \n",
    "        inputs = channels\n",
    "        \n",
    "        width = self.input_size\n",
    "        self.encoder_layer_output_widths = {}\n",
    "        encoder_modules = []\n",
    "        for i in range(self.layer_count):\n",
    "            mul *= self.depth\n",
    "            out_size = mul\n",
    "            encoder_modules.append(self.encode_unit(inputs, out_size, self.kernel_size, self.stride, padding=self.padding, dropout=self.encode_dropout))\n",
    "            inputs = out_size\n",
    "            \n",
    "            width = (width + 2 * self.padding - 1 * (self.kernel_size - 1) - 1) / self.stride + 1\n",
    "            self.encoder_layer_output_widths[i] = width\n",
    "            \n",
    "            \n",
    "        self.encoder_end_width = int(np.floor(width))\n",
    "        \n",
    "            \n",
    "        if fc_size == None:\n",
    "            fc_size = int(mul * np.floor(width) ** 2)\n",
    "        self.fc_size = fc_size\n",
    "        \n",
    "        # Build Encoder\n",
    "        self.encoder = nn.Sequential(*encoder_modules)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(self.fc_size, latent_dim)\n",
    "        self.fc_var = nn.Linear(self.fc_size, latent_dim)\n",
    "\n",
    "        \n",
    "        self.decoder_input = nn.Linear(latent_dim, self.fc_size)\n",
    "        \n",
    "        mul = inputs / depth\n",
    "#         print(f'decode start mul = {mul}')\n",
    "        decoder_modules = []\n",
    "        for i in range(self.layer_count):\n",
    "            \n",
    "            out_size = int(mul)\n",
    "            print(f'decoder module {i} out_size = {out_size}')\n",
    "            if i == (self.layer_count -1):\n",
    "                output_padding = 1\n",
    "            else:\n",
    "                output_padding = 0\n",
    "            decoder_modules.append(self.decode_unit(inputs, out_size, self.kernel_size, self.stride, padding=self.padding, dropout=self.decode_dropout, output_padding=output_padding))\n",
    "            mul = mul / depth\n",
    "            inputs = out_size\n",
    "            \n",
    "        self.decoder = nn.Sequential(*decoder_modules)\n",
    "        \n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, int(self.depth ** self.layer_count), self.encoder_end_width, self.encoder_end_width)\n",
    "        result = self.decoder(result)\n",
    "        # result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        mu = mu.squeeze()\n",
    "        logvar = logvar.squeeze()\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "#         return self.decode(z.view(-1, self.zsize, 1, 1)), x, mu, logvar\n",
    "        return  [self.decode(z), x, mu, logvar]\n",
    "\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "            \n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        device = self.dummy_param.device\n",
    "        \n",
    "        recons = args[0]\n",
    "        inputs = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight = self.kld_loss_weight # Account for the minibatch samples from the dataset\n",
    "        recons_loss = self.mse_loss(recons, inputs)\n",
    "\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 0), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_loss * kld_weight\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss, 'KLD':kld_loss}\n",
    "    \n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop.\n",
    "        # It is independent of forward\n",
    "        x = batch\n",
    "        x_hat, inputs, mu, log_var = self(x)\n",
    "        vae_loss = self.loss_function(x_hat, inputs, mu, log_var)\n",
    "        \n",
    "        self.log('train_x_loss', vae_loss['loss'], on_step=True, prog_bar=False)\n",
    "        self.log('train_x_recon_loss', vae_loss['Reconstruction_Loss'], on_step=True, prog_bar=False)\n",
    "        self.log('train_kld_loss', vae_loss['KLD'], on_step=True, prog_bar=False)\n",
    "        loss = vae_loss['loss']\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x_hat, inputs, mu, log_var = self(x)\n",
    "        vae_loss = self.loss_function(x_hat, inputs, mu, log_var)\n",
    "        self.log('val_x_loss', vae_loss['loss'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_x_recon_loss', vae_loss['Reconstruction_Loss'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_kld_loss', vae_loss['KLD'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "        loss = vae_loss['loss']\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x_hat, inputs, mu, log_var = self(x)\n",
    "        vae_loss = self.loss_function(x_hat, inputs, mu, log_var)\n",
    "        self.log('test_x_loss', vae_loss['loss'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_x_recon_loss', vae_loss['Reconstruction_Loss'], on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log('test_kld_loss', vae_loss['KLD'], on_step=False, on_epoch=True, prog_bar=False)\n",
    "        loss = vae_loss['loss']\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71953b3f-f521-4681-be73-b8bc2e2a3372",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPVAE(pl.LightningModule):\n",
    "    \n",
    "    def encode_unit(self, inputs, out_size, kernel_size, stride, padding=0, dropout=0):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(inputs, out_size, kernel_size, stride, padding),\n",
    "            nn.BatchNorm2d(out_size, track_running_stats=False, affine=False),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "    \n",
    "    def decode_unit(self, inputs, out_size, kernel_size, stride, padding=0, dropout=0, output_padding=0):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(inputs, out_size, kernel_size, stride, padding, output_padding=output_padding),\n",
    "            nn.BatchNorm2d(out_size, track_running_stats=False, affine=False),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def fc_bn_lrelu(self, in_size, out_size, dropout=0):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_size, out_size),\n",
    "            nn.BatchNorm1d(out_size, track_running_stats=False, affine=False),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        \n",
    "        \n",
    "        latent_dim: int,\n",
    "        input_size=(256, 256),\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-5,\n",
    "        hidden_dims: List = None,\n",
    "        kld_loss_weight=1.,\n",
    "        input_dropout=0.,\n",
    "        encoder_dropout=0.,\n",
    "        decoder_dropout=0.,\n",
    "        do_test_step=True,\n",
    "        \n",
    "    ):\n",
    "        super(MLPVAE, self).__init__()\n",
    "\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.weight_decay = weight_decay\n",
    "        self.kld_loss_weight = kld_loss_weight\n",
    "        \n",
    "        self.encoder_dropout = encoder_dropout\n",
    "        self.decoder_dropout = decoder_dropout\n",
    "        self.input_size = input_size\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
    "        self.n_features = self.input_size[0] * self.input_size[1]\n",
    "        \n",
    "        \n",
    "        if hidden_dims == None:\n",
    "            hidden_dims = [32, 16, 10]\n",
    "        \n",
    "        self.hidden_dims = hidden_dims\n",
    "        \n",
    "        self.input_dropout = nn.Dropout(input_dropout)\n",
    "        \n",
    "        \n",
    "        # Build Encoder\n",
    "        self.encoder_dims = [self.n_features] + self.hidden_dims\n",
    "        self.encoder_modules = []\n",
    "        for ii in range(len(self.encoder_dims) - 1):\n",
    "            input_d = self.encoder_dims[ii]\n",
    "            output_d = self.encoder_dims[ii+1]\n",
    "            self.encoder_modules.append(self.fc_bn_lrelu(input_d, output_d, dropout=self.encoder_dropout))\n",
    "        self.encoder = nn.Sequential(*self.encoder_modules)\n",
    "        self.fc_mu = nn.Linear(self.encoder_dims[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(self.encoder_dims[-1], latent_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "        self.decoder_modules = []\n",
    "        self.decoder_dims = list(reversed(self.encoder_dims))[:-1]\n",
    "        self.decoder_input = nn.Linear(latent_dim, self.decoder_dims[0])\n",
    "        self.decoder_modules = []\n",
    "        \n",
    "        for ii in range(len(self.decoder_dims) - 1):\n",
    "            input_d = self.decoder_dims[ii]\n",
    "            output_d = self.decoder_dims[ii+1]\n",
    "            self.decoder_modules.append(self.fc_bn_lrelu(input_d, output_d, dropout=self.decoder_dropout))\n",
    "\n",
    "#         self.decoder_modules.append(\n",
    "#             self.fc_bn_lrelu(self.decoder_dims[-1], self.n_features)\n",
    "#         )\n",
    "        self.decoder_modules.append(\n",
    "            nn.Sequential(\n",
    "            nn.Linear(self.decoder_dims[-1], self.n_features),\n",
    "            nn.PReLU(),\n",
    "        ))\n",
    "        self.decoder = nn.Sequential(*self.decoder_modules)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input.flatten(start_dim=1))\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        \n",
    "        result = self.decoder(result).unflatten(dim=1, sizes=self.input_size).unsqueeze(1)\n",
    "        # result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        mu = mu.squeeze()\n",
    "        logvar = logvar.squeeze()\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "#         return self.decode(z.view(-1, self.zsize, 1, 1)), x, mu, logvar\n",
    "        return  [self.decode(z), x, mu, logvar]\n",
    "\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "            \n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        device = self.dummy_param.device\n",
    "        \n",
    "        recons = args[0]\n",
    "        inputs = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight = self.kld_loss_weight # Account for the minibatch samples from the dataset\n",
    "        recons_loss = self.mse_loss(recons, inputs)\n",
    "\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 0), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_loss * kld_weight\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss, 'KLD':kld_loss}\n",
    "    \n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defined the train loop.\n",
    "        # It is independent of forward\n",
    "        x = batch\n",
    "        x_hat, inputs, mu, log_var = self(x)\n",
    "        vae_loss = self.loss_function(x_hat, inputs, mu, log_var)\n",
    "        \n",
    "        self.log('train_x_loss', vae_loss['loss'], on_step=True, prog_bar=False)\n",
    "        self.log('train_x_recon_loss', vae_loss['Reconstruction_Loss'], on_step=True, prog_bar=False)\n",
    "        self.log('train_kld_loss', vae_loss['KLD'], on_step=True, prog_bar=False)\n",
    "        loss = vae_loss['loss']\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x_hat, inputs, mu, log_var = self(x)\n",
    "        vae_loss = self.loss_function(x_hat, inputs, mu, log_var)\n",
    "        self.log('val_x_loss', vae_loss['loss'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_x_recon_loss', vae_loss['Reconstruction_Loss'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_kld_loss', vae_loss['KLD'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "        loss = vae_loss['loss']\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x_hat, inputs, mu, log_var = self(x)\n",
    "        vae_loss = self.loss_function(x_hat, inputs, mu, log_var)\n",
    "        self.log('test_x_loss', vae_loss['loss'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_x_recon_loss', vae_loss['Reconstruction_Loss'], on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log('test_kld_loss', vae_loss['KLD'], on_step=False, on_epoch=True, prog_bar=False)\n",
    "        loss = vae_loss['loss']\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd343a-d2b3-4f74-aa84-479f5de81edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ImageDataModule(\n",
    "    np.stack(imgs), \n",
    "    batch_size=32,\n",
    "    num_workers=48,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "x = next(iter(dm.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da8717-9df9-4946-9225-6f6c61c3e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae = MLPVAE(\n",
    "#     latent_dim=256,\n",
    "#     input_size=(128,128),\n",
    "    \n",
    "#     lr=1e-3,\n",
    "#     weight_decay=1e-5,\n",
    "    \n",
    "#     encoder_dropout=0.4,\n",
    "#     kld_loss_weight=1e-3,\n",
    "#     hidden_dims=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d93d8f-7a4c-417c-ad6d-116df759f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_hat, inputs, mu, log_var = vae(x)\n",
    "# print(x_hat.shape)\n",
    "# print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871bed0-1c38-4df3-ba88-d07ef6b4ea67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d70b2-967b-4893-b84d-744bfe1a8d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = '052_ode_vae.ipynb'\n",
    "run = wandb.init(\n",
    "    project='ode_vae', \n",
    "    entity='alex_naka', \n",
    "#     mode='disabled',\n",
    ")\n",
    "\n",
    "pl.seed_everything(117)\n",
    "\n",
    "callbacks = []\n",
    "earlystopper = EarlyStopping(monitor='val_x_recon_loss',patience=300,mode='min')\n",
    "callbacks += [earlystopper]\n",
    "\n",
    "logger = WandbLogger( )\n",
    "\n",
    "dm = ImageDataModule(\n",
    "    np.stack(imgs), \n",
    "    batch_size=16,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "vae = MLPVAE(\n",
    "    latent_dim=64,\n",
    "    input_size=(128, 128),\n",
    "    \n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-14,\n",
    "    \n",
    "    encoder_dropout=0.05,\n",
    "    kld_loss_weight=1e-5,\n",
    "    hidden_dims=[256, 64, 64],\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(       \n",
    "        gpus=1,\n",
    "        max_epochs=1000, \n",
    "        progress_bar_refresh_rate=50,\n",
    "        logger=logger, \n",
    "        callbacks=callbacks,\n",
    "        \n",
    "        )\n",
    "\n",
    "trainer.fit(vae, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622efe5-95c8-4964-82b7-dff8d3ba56b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9986b3e-5167-4549-952e-538eeea0591f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "x = next(iter(dm.train_dataloader()))\n",
    "ii = np.random.randint(len(x))\n",
    "f,axs = plt.subplots(1,2,figsize=(16,8))\n",
    "ax = axs[0]\n",
    "ax.imshow(dm.train_data[ii].detach().numpy().squeeze())\n",
    "ax = axs[1]\n",
    "img_hat = vae(x)[0]\n",
    "ax.imshow(img_hat[ii].detach().cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459bc96-cad1-4a99-a9ad-32387a3e6b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "x = next(iter(dm.val_dataloader()))\n",
    "ii = np.random.randint(len(x))\n",
    "f,axs = plt.subplots(1,2,figsize=(16,8))\n",
    "ax = axs[0]\n",
    "ax.imshow(x[ii].detach().numpy().squeeze())\n",
    "ax = axs[1]\n",
    "img_hat = vae(x)[0]\n",
    "ax.imshow(img_hat[ii].detach().cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e000994-f7bf-411b-ba7b-e152c49a107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b4a2e6-3cd5-4462-8716-5e01750b853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axs = plt.subplots(4, 4,figsize=(16,16))\n",
    "axs = axs.ravel()\n",
    "img_samples = vae.sample(16, vae.device)\n",
    "for ii, img_sample in enumerate(img_samples):\n",
    "    ax = axs[ii]\n",
    "    ax.imshow(img_sample.detach().cpu().numpy().squeeze())\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7869a-6ea7-4a5b-a3a8-437166bd20fd",
   "metadata": {},
   "source": [
    "images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84060bdc-054a-4a9d-8c02-7d295c4cc924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d058060-4502-4689-b5fb-a8d5423c28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_shape=(256, 256)\n",
    "border = 5\n",
    "img_drawbox = box(border, border, out_shape[0]-border, out_shape[1]-border)\n",
    "xmin, xmax = (-18, 18)\n",
    "ymin, ymax = (-18, 18)\n",
    "xrange = np.linspace(xmin, xmax, out_shape[0])\n",
    "yrange = np.linspace(ymin, ymax, out_shape[1])\n",
    "xgs, ygs = np.meshgrid(xrange, yrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a9e89a-1174-4f05-b078-e8678ff92c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = OdeParams().get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aea1b8-3fa7-45bf-882e-b88ffe080850",
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aecdb2d-50d7-4b2a-8a91-ed4677aec7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx, dy = ode((xg,yg), 0, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6dacde-b976-41f1-902a-271e3f9eede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dxs = []\n",
    "dys = []\n",
    "angles = []\n",
    "mags = []\n",
    "for xg, yg in zip(xgs.ravel(), ygs.ravel()):\n",
    "    dx, dy = ode((xg,yg), 0, *args)\n",
    "    dxs.append(dx)\n",
    "    dys.append(dy)\n",
    "    angles.append(np.arctan2(dx, dy))\n",
    "    mags.append(np.sqrt(dx **2 + dy **2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90525eb3-ee93-4d58-84e5-d5489f6d1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(mags).reshape(*out_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4184bf-49d8-4aaf-b45e-960d01c0392d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for ii in tqdm(range(20000)):\n",
    "#     args = OdeParams().get_args()\n",
    "#     lss = no_overlap_odeint(ode, pts, args, t, break_dist=0.5, min_len=0.9)\n",
    "#     lss = gp.make_like(gp.merge_LineStrings(lss), img_drawbox)\n",
    "#     img = features.rasterize(lss, out_shape=out_shape)\n",
    "#     linesets.append(lss)\n",
    "#     imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5a2a7-9c43-4e50-ae4c-786cf2aa7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez_compressed('/home/naka/data/ode_vae/test.npz', imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1168df-12b7-40a3-a0ed-d78ed8b5eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.load('/home/naka/data/ode_vae/test.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274345b2-3964-4302-8ab6-b3b8fe4f66a8",
   "metadata": {},
   "source": [
    "# new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a50c6-20f5-45ff-a462-18c7d58dd989",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_shape=(128, 128)\n",
    "border = 0\n",
    "img_drawbox = box(border, border, out_shape[0]-border, out_shape[1]-border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327348a0-a557-4421-a25a-1b3fd28ba956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs = []\n",
    "n_images=2000\n",
    "\n",
    "for ii in tqdm(range(n_images)):\n",
    "    n_shapes = np.random.randint(1,2)\n",
    "    xs = np.linspace(0, 6, n_shapes) + np.random.uniform(-2,2, n_shapes)\n",
    "    ys = np.linspace(0, 6, n_shapes) + np.random.uniform(-2,2, n_shapes)\n",
    "\n",
    "    points = []\n",
    "    for x in xs:\n",
    "        for y in ys:\n",
    "            points.append(Point(x,y))\n",
    "\n",
    "    polys = [gp.RegPolygon(p, radius=2, n_corners=np.random.randint(3,8), rotation=np.random.uniform(180)).poly for p in points]\n",
    "    polys = gp.make_like(gp.merge_Polygons(polys), img_drawbox)\n",
    "    poly = so.unary_union(polys)\n",
    "\n",
    "    prms = gp.ScaleTransPrms(\n",
    "        d_buffer=np.random.uniform(-7.4, -2.8),\n",
    "#         d_buffer=-1.2,\n",
    "        n_iters=400,\n",
    "        d_translate_factor=0.7,\n",
    "        angles=np.random.uniform(0,180),\n",
    "    )\n",
    "\n",
    "    poly = gp.Poly(poly)\n",
    "\n",
    "    poly.fill_scale_trans(**prms.prms)\n",
    "    lss = poly.fill\n",
    "    lss = gp.make_like(gp.merge_LineStrings(lss), img_drawbox)\n",
    "    img = features.rasterize(lss, out_shape=out_shape, fill=np.random.randint(1,10), default_value=np.random.randint(10,100))\n",
    "#     img = features.rasterize(polys, out_shape=out_shape, fill=10)\n",
    "#     linesets.append(lss)\n",
    "    imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d885fbf-6981-453b-8c70-64617e771863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(10,10, figsize=(18,18))\n",
    "axs = axs.ravel()\n",
    "for ii in range(100):\n",
    "    ax = axs[ii]\n",
    "    ax.imshow(imgs[ii])\n",
    "    ax.axis('off')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae45edf-b570-4587-862b-584a6dd4aef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37474fb-5919-4d2a-9353-b397e2f72d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('/home/naka/data/ode_vae/hatch_fill_polys.npz', imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ed7e9c-4422-4eca-a998-f9cfa979e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.load('/home/naka/data/ode_vae/hatch_fill_polys.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c7651-fbf7-4d3b-9cdb-d4f57cfde62b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = '052_ode_vae.ipynb'\n",
    "run = wandb.init(\n",
    "    project='hatch_fill_poly_blob_vae', \n",
    "    entity='alex_naka', \n",
    "#     mode='disabled',\n",
    ")\n",
    "\n",
    "pl.seed_everything(117)\n",
    "\n",
    "callbacks = []\n",
    "earlystopper = EarlyStopping(monitor='val_x_recon_loss',patience=300,mode='min')\n",
    "callbacks += [earlystopper]\n",
    "\n",
    "logger = WandbLogger( )\n",
    "\n",
    "dm = ImageDataModule(\n",
    "    np.stack(imgs), \n",
    "    batch_size=128,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "vae = MLPVAE(\n",
    "    latent_dim=24,\n",
    "    input_size=(128, 128),\n",
    "    \n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-14,\n",
    "    \n",
    "    encoder_dropout=0.1,\n",
    "    kld_loss_weight=1e-7,\n",
    "    hidden_dims=[256, 64],\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(       \n",
    "        gpus=1,\n",
    "        max_epochs=1000, \n",
    "        progress_bar_refresh_rate=50,\n",
    "        logger=logger, \n",
    "        callbacks=callbacks,\n",
    "        \n",
    "        )\n",
    "\n",
    "trainer.fit(vae, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21888c-ba8b-4c0a-b73e-e3d4c4bfcdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ImageDataModule(\n",
    "    np.stack(imgs), \n",
    "    batch_size=128,\n",
    "    num_workers=48,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "x = next(iter(dm.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb283b-3aef-4fec-bd64-5d8d8441aa98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vae = VAE(\n",
    "    latent_dim=32,\n",
    "    input_size=128,\n",
    "    layer_count=3, \n",
    "    channels=1, \n",
    "    depth=2,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-9,\n",
    "    kld_loss_weight=1e-3,\n",
    "    mul=1,\n",
    "    fc_scale=8,\n",
    "    encode_dropout=0.0,\n",
    "    decode_dropout=0.0,\n",
    "    kernel_size=3,\n",
    "    stride=2,\n",
    "    fc_size=1800,\n",
    "    padding=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b413e-ba9b-432b-bd85-05351fbd8857",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat, inputs, mu, log_var = vae(x)\n",
    "print(x_hat.shape)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7102ac-f8b2-4740-aa3c-1871befd9678",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['WANDB_NOTEBOOK_NAME'] = '052_ode_vae.ipynb'\n",
    "run = wandb.init(\n",
    "    project='hatch_fill_poly_blob_vae', \n",
    "    entity='alex_naka', \n",
    "#     mode='disabled',\n",
    ")\n",
    "\n",
    "pl.seed_everything(117)\n",
    "\n",
    "callbacks = []\n",
    "earlystopper = EarlyStopping(monitor='val_x_recon_loss',patience=300,mode='min')\n",
    "callbacks += [earlystopper]\n",
    "\n",
    "logger = WandbLogger( )\n",
    "\n",
    "dm = ImageDataModule(\n",
    "    np.stack(imgs), \n",
    "    batch_size=128,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "vae = VAE(\n",
    "    latent_dim=32,\n",
    "    input_size=128,\n",
    "    layer_count=3, \n",
    "    channels=1, \n",
    "    depth=2,\n",
    "    lr=1e-2,\n",
    "    weight_decay=1e-9,\n",
    "    kld_loss_weight=1e-7,\n",
    "    mul=1,\n",
    "    fc_scale=8,\n",
    "    encode_dropout=0.0,\n",
    "    decode_dropout=0.0,\n",
    "    kernel_size=3,\n",
    "    stride=2,\n",
    "    fc_size=1800,\n",
    "    padding=0,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(       \n",
    "        gpus=1,\n",
    "        max_epochs=1000, \n",
    "        progress_bar_refresh_rate=50,\n",
    "        logger=logger, \n",
    "        callbacks=callbacks,\n",
    "        \n",
    "        )\n",
    "\n",
    "trainer.fit(vae, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c80e1cf-4214-4114-89fb-b0d4f8351ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "x = next(iter(dm.train_dataloader()))\n",
    "ii = np.random.randint(len(x))\n",
    "img_hat = vae(x)\n",
    "f,axs = plt.subplots(1,2,figsize=(16,8))\n",
    "ax = axs[0]\n",
    "ax.imshow(img_hat[1][ii].detach().cpu().numpy().squeeze())\n",
    "ax = axs[1]\n",
    "\n",
    "ax.imshow(img_hat[0][ii].detach().cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f4712-9ed4-4ccf-9f8b-a958f2ad89c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "x = next(iter(dm.val_dataloader()))\n",
    "ii = np.random.randint(len(x))\n",
    "f,axs = plt.subplots(1,2,figsize=(16,8))\n",
    "ax = axs[0]\n",
    "ax.imshow(x[ii].detach().numpy().squeeze())\n",
    "ax = axs[1]\n",
    "img_hat = vae(x)[0]\n",
    "ax.imshow(img_hat[ii].detach().cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff700ce4-0624-4540-9690-d23d25da92cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9298d0-2931-4a83-aec8-f17c33c1249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axs = plt.subplots(4, 4,figsize=(16,16))\n",
    "axs = axs.ravel()\n",
    "img_samples = vae.sample(16, vae.device)\n",
    "for ii, img_sample in enumerate(img_samples):\n",
    "    ax = axs[ii]\n",
    "    ax.imshow(img_sample.detach().cpu().numpy().squeeze())\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318edea7-195d-4f67-8f99-7ac680a17775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87535e27-5125-4798-81c2-d0f673b8946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples=2\n",
    "z = torch.randn(num_samples,\n",
    "                        vae.latent_dim)\n",
    "_img = vae.decode(z)[0]\n",
    "plt.imshow(_img.detach().cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4eb0b6-6649-4bac-b95e-8413b441475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z[0][0]+ 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8526f5f-8f87-4309-922c-bc948a62f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.latent_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b9df0-f74d-427f-84c3-85ff44ac5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_interps=10\n",
    "\n",
    "f,axs = plt.subplots(vae.latent_dim, n_interps,figsize=(1.2*n_interps, 1.2*vae.latent_dim,))\n",
    "for ii, dim in enumerate(range(vae.latent_dim)):\n",
    "    for jj, zp in enumerate(np.linspace(-4.5, 4.5, n_interps)):\n",
    "        ax = axs[ii, jj]\n",
    "        _z = z.clone()\n",
    "        _z[0][dim] += zp\n",
    "        _img = vae.decode(_z)[0]\n",
    "        ax.imshow(_img.detach().cpu().numpy().squeeze())\n",
    "        ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e404bd-76bb-4717-be05-9974d179bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples=2\n",
    "z = torch.randn(num_samples,\n",
    "                        vae.latent_dim)\n",
    "_img = vae.decode(z)[0]\n",
    "plt.imshow(_img.detach().cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a720da57-0c9e-4cf8-8825-3425635aa321",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.latent_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002ff56-1634-4f6a-a746-53f1762854ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_walks = 64\n",
    "_z = z.clone()\n",
    "f,axs = plt.subplots(8,8,figsize=(12,12))\n",
    "axs= axs.ravel()\n",
    "for ii in range(n_walks):\n",
    "    ax = axs[ii]\n",
    "    _z += torch.randn(num_samples,\n",
    "                        vae.latent_dim)*0.25\n",
    "    _img = vae.decode(_z)[0]\n",
    "    ax.imshow(_img.detach().cpu().numpy().squeeze())\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535ae068-ed71-4ec8-aff2-d6722134a413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:genpen]",
   "language": "python",
   "name": "conda-env-genpen-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
